# -*- coding: utf-8 -*-
"""Heart Attack ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W92_I5pRD8UHyFU0YOxjqjr0iqHT-Me6
"""

!pip install pandas

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import sklearn
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
plt.rcParams["figure.figsize"] = [10,5]

!pip install -U ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
heart_disease = fetch_ucirepo(id=45)

# data (as pandas dataframes)
X = heart_disease.data.features
y = heart_disease.data.targets

# metadata
print(heart_disease.metadata)

# variable information
print(heart_disease.variables)

full_data = pd.read_csv('https://archive.ics.uci.edu/static/public/45/data.csv')

"""To download the file"""

import pandas as pd

# Assuming df is your cleaned DataFrame
full_data.to_csv('cleaned_data.csv', index=False)

full_data.head(5)

full_data.corr()

# Heatmap
sns.heatmap(full_data.isnull(),yticklabels = False, cbar = False,cmap = 'tab20c_r')
plt.title('Missing Data: Training Set')
plt.show()

full_data.shape

"""**Cleaning the data**"""

#Check the duplicates values
duplicate_rows_df = full_data[full_data.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)

#Counting the number of Rows
full_data.count()

#Dropping the Duplicate Values
df = full_data.drop_duplicates()
df.head(5)

df.count()

#Dropping the missing or null values.
print(df.isnull().sum())

df = df.dropna()    # Dropping the missing values.

df.count()

df.head(5)

#Calculate basic summary statistics (mean, median, min, max, etc.) for the numerical
#features. Using .describe()
df.describe()

"""## **OBJECTIVE 2: MACHINE LEARNING**
Next, I will feed these features into various classification algorithms to determine the best performance using a simple framework: **Split, Fit, Predict, Score It.**
"""

# Split data to be used in the models
# Create matrix of features
# X is input
x = df.drop('num', axis = 1) # grabs everything else but 'Outcome'

# Create target variable
# Y is output
y = df['num'] # y is the column we're trying to predict

# Use x and y variables to split the training data into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .20, random_state = 100)

"""**Model Training**

---


"""

#Fit
# Import model
from sklearn.linear_model import LogisticRegression

# Create instance of model
lreg = LogisticRegression()

# Pass training data into model
lreg.fit(x_train, y_train)

# Predict
y_pred_lreg = lreg.predict(x_test)
print(y_pred_lreg)

"""**Check the accuracy of the model**"""

# Score It (to check accuracy)
from sklearn.metrics import classification_report, accuracy_score

print('Classification Model')
# Accuracy
print('--'*30)
logreg_accuracy = round(accuracy_score(y_test, y_pred_lreg) * 100,2)
print('Accuracy', logreg_accuracy,'%')