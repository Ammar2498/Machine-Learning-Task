# -*- coding: utf-8 -*-
"""credit card fraud detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QkK_-hT1z9mUv8zxFazqZjPlxS0Hjo7
"""

!pip install pandas

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import sklearn
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
plt.rcParams["figure.figsize"] = [10,5]

full_data=pd.read_csv('/content/creditcard.csv')

full_data.head(5)

full_data.tail(5)

full_data.shape

full_data.info()

"""###check the missing values by using heatmap"""

# Heatmap
sns.heatmap(full_data.isnull(),yticklabels = False, cbar = False,cmap = 'tab20c_r')
plt.title('Missing Data: Training Set')
plt.show()

"""###Check the five number summary"""

# number summary
full_data.describe()

print(full_data.isnull().sum())

full_data.dropna(inplace = True)

print(full_data.isnull().sum())

full_data.count()

"""###Check the Duplicate Values"""

duplicate_rows_df = full_data[full_data.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)

"""###droping the Duplicate rows"""

full_data = full_data.drop_duplicates()
full_data.head(5)

duplicate_rows_df = full_data[full_data.duplicated()]
print("number of duplicate rows: ", duplicate_rows_df.shape)

# number summary
full_data.describe()

full_data.corr()

"""

<a id = "Matrix"></a>
## 6. Matrix plots

<a id = "heatmap"></a>
### 6.1. heatmap

Visualizing data with **heatmaps** is a great way to do exploratory data analysis, when you have a data set with multiple variables. Heatmaps can reveal general pattern in the dataset, instantly. And it is very easy to make beautiful heatmaps with Seaborn library in Python.

Now let's plot the correlation matrix of our data with a heatmap."""

plt.subplots(figsize=(20, 20))
sns.heatmap(full_data.corr(), cmap = "YlGnBu", annot=True, fmt=".2f")
plt.show()

"""OBJECTIVE 2: MACHINE LEARNING
Next, I will feed these features into various classification algorithms to determine the best performance using a simple framework: Split, Fit, Predict, Score It.

## **Target Variable Splitting**
We will spilt the Full dataset into **Input** and **target** variables

Input is also called **Feature Variables**
Output referes to Target **variables**
"""

full_data.head(6)

# Split data to be used in the models
# Create matrix of features
x = full_data.drop('Class', axis = 1) # grabs everything else but 'Survived'

# Create target variable
y = full_data['Class'] # y is the column we're trying to predict

# Adjusting the Size of Figure
plt.figure(figsize=(30,20))
# calculating the Correlation
correlation = full_data.corr()
# Displaying the correlation using the Heap Map
sns.heatmap(correlation,cmap="BrBG",annot=True) # Br: Brown. B: Blue, G: Green

#correlation

x.shape

y.shape

# Use x and y variables to split the training data into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .30, random_state = 100)

print("Shape of x_train: ",x_train.shape)
print("Shape of y_train: ",y_train.shape)
print("---"*10)
print("Shape of x_test: ",x_test.shape)
print("Shape of y_test: ",y_test.shape)

"""# **1. LOGISTIC REGRESSION**

## **Model Training**
"""

# Import model
from sklearn.linear_model import LogisticRegression

print('Logistic Regression')
# Create instance of model
log_reg = LogisticRegression()

# Pass training data into model
log_reg.fit(x_train, y_train)

"""##Model Evaluation"""

#logistic Regression

from sklearn.metrics import accuracy_score
# prediction from the model
y_pred_log_reg = log_reg.predict(x_test)
# Score It

print('Logistic Regression')
# Accuracy
print('--'*30)
log_reg_accuracy = round(accuracy_score(y_test, y_pred_log_reg) * 100,2)
print('Accuracy', log_reg_accuracy,'%')

from sklearn.metrics import precision_score, recall_score, confusion_matrix
# Calculate precision and recall
precision = precision_score(y_test, y_pred_log_reg)
recall = recall_score(y_test, y_pred_log_reg)

# Print the results
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print("--"*30)
# Calculate confusion matrix
confusion = confusion_matrix(y_test, y_pred_log_reg)
print(confusion)
sns.heatmap(confusion, annot=True, fmt="d")